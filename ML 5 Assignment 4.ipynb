{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0321869e-825f-44ff-9109-87c44fd7f9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " 0\n",
      "Data loaded and scaled successfully.\n"
     ]
    }
   ],
   "source": [
    "#1.Loading and Preprocessing\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Loading the dataset\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "y=pd.Series(data.target)\n",
    "#Checking for any missing values\n",
    "print(\"Missing values:\\n\",X.isnull().sum().sum())\n",
    "#Feature Scaling\n",
    "scaler=StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X)\n",
    "print(\"Data loaded and scaled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0bdae1-838e-45fc-80fc-0394b2957b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explanation for the above step \n",
    "#Missing values:Checked and found that no missing values exist in this dataset.\n",
    "#Feature Scaling:Used StandardScaler method to normalize the data.It is required especially for SVM and K-NN, which are sensitive to scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f38a46-3b0e-4783-8d5c-91043bb4e102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.973684   0.972222  0.985915  0.979021\n",
      "3                  SVM  0.973684   0.972222  0.985915  0.979021\n",
      "2        Random Forest  0.964912   0.958904  0.985915  0.972222\n",
      "1        Decision Tree  0.947368   0.957746  0.957746  0.957746\n",
      "4                 K-NN  0.947368   0.957746  0.957746  0.957746\n"
     ]
    }
   ],
   "source": [
    "#2.Classification Algorithm Implementation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "#Training test splitting\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_scaled,y,test_size=0.2,random_state=42)\n",
    "#Defining the models\n",
    "models={\n",
    "    \"Logistic Regression\":LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\":DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\":RandomForestClassifier(random_state=42),\n",
    "    \"SVM\":SVC(),\n",
    "    \"K-NN\":KNeighborsClassifier()\n",
    "}\n",
    "results=[]\n",
    "#Training and evaluating each model\n",
    "for name,model in models.items():\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    acc=accuracy_score(y_test,y_pred)\n",
    "    prec=precision_score(y_test,y_pred)\n",
    "    rec=recall_score(y_test,y_pred)\n",
    "    f1=f1_score(y_test,y_pred)\n",
    "    results.append({\n",
    "        \"Model\":name,\n",
    "        \"Accuracy\":acc,\n",
    "        \"Precision\":prec,\n",
    "        \"Recall\":rec,\n",
    "        \"F1 Score\":f1\n",
    "    })\n",
    "results_df=pd.DataFrame(results)\n",
    "print(results_df.sort_values(by=\"Accuracy\",ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1f38a79-89e5-46a7-974e-8699efb52ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Description\n",
    "#Logistic Regression:A linear model for binary classification which is fast and interpretable.\n",
    "#Decision Tree:Splits the data into several branches based on feature thresholds and is easy to interpret but may overfit.\n",
    "#Random Forest:Ensemble of decision trees and reduces overfitting and performs well on many tasks.\n",
    "#Support Vector Machine(SVM):Finds a hyperplane that can best separates classes ans is effective in high-dimensional space.\n",
    "#KNN(k-Nearest Neighbors):Classifies based on majority vote from k nearest neighbours. It is simple,but slower on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f42510-0753-4f49-9a99-4ca2a088dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Model Comparison\n",
    "#Best Performing algorithm:Random Forest or SVM.\n",
    "#Worst Performing algorithm: Often Decision Tree or KNN,which is depending on overfitting or scaling sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851495c3-1320-4d18-b147-7d5b715ff969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
